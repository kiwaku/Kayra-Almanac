---
title: "Dual ConnectX-3 Pro Operational: PCIe Topology & DAC Loop Validated"
date: "2026-02-09"
tags: ["hardware", "hephaestus", "pcie", "infiniband", "connectx-3"]
project: hephaestus
images: []
anomaly: false
---

Both Mellanox ConnectX-3 Pro HCAs (MT27520, fw 2.38.5000) enumerated and validated with a QSFP+ DAC loop. PCIe topology confirmed, link speeds characterized, jumbo MTU clean.

<br />

### PCIe topology (known-good state)

| HCA | BDF | Upstream Port | Link Negotiated | Notes |
|---|---|---|---|---|
| #1 (native slot) | `0000:03:00.0` | `0000:00:03.0` (CPU root complex) | **8GT/s x8** (Gen3 x8) | Full speed, no downgrade |
| #2 (right M.2 via riser) | `0000:09:00.0` | `0000:00:1c.4` (PCH root port) | **5GT/s x4** (Gen2 x4) | Downgraded — caps real throughput |

**Left M.2** does not reliably enumerate the HCA — either not NVMe-wired the same way, shares lanes with another device, or is electrically marginal with this riser/card combo. **Right M.2 is the only stable path.**

Anti-amnesia topology block:
- `03:00.0` = native slot = Gen3 x8
- `09:00.0` = right M.2 via `00:1c.4` = Gen2 x4
- `ens1` ↔ `enp9s0`, `ens1d1` ↔ `enp9s0d1`

---

<br />

### Bandwidth reality

- PCIe Gen2 x4 raw is ~20 Gbit/s, practical lower after overhead
- M.2-attached HCA caps iperf3 multistream at **~11–12 Gbit/s** — PCIe link width/speed is the limiter, not the DAC
- Good enough for: stability testing, RDMA plumbing, topology/passthrough work, burn-in, 10–20G class networking
- **Not** good enough for: true 40GbE line-rate validation or stressing the card at its real ceiling

---

<br />

### DAC loop validation

Initial "link detected: no" traced to wrong port-to-port wiring — **port 1 must connect to port 1, port 2 to port 2**. After correction, all four netdevs show 40,000 Mb/s link detected: yes.

Connectivity validated inside **network namespaces** (isolated from host bridges) using `198.18.0.0/15` (benchmark/testing range):
- Set MTU 9000 on both ends
- DF pings at 1500 / 4000 / 9000 payload sizes — all succeeded, 0% loss, sub-ms RTT

---

<br />

### Driver state (known-good)

- **Driver:** mlx4_en (Ethernet mode), mlx4_ib + ib_core + ib_uverbs loaded
- **Firmware:** 2.38.5000 on both
- **ibv_devinfo:** link_layer: Ethernet, ports down until interfaces brought up

```
HCA 0000:03:00.0
  ens1   → port 0
  ens1d1 → port 1

HCA 0000:09:00.0
  enp9s0   → port 0
  enp9s0d1 → port 1
```

---

<br />

### Chipset root port detail

With HCA#2 in the right M.2, the platform exposes `0000:00:1c.4` with bus `[09]` in `lspci -t`. When HCA#2 was in the left M.2, `00:1c.4` did not show as present — that downstream bus was occupied by a Realtek NIC on `08:00.0` under `00:1c.3`.

Root port verification:

```bash
sudo lspci -s 00:1c.4 -vv | egrep -i "LnkCap|LnkSta|Width|Speed"
```

- **LnkCap:** Speed 5GT/s, Width x4
- **LnkSta:** Speed 5GT/s, Width x1 (with mining riser) / Width x4 (with proper M.2 adapter)

The PCH root port can do Gen2 x4. The mining riser only presented x1 lanes; the current adapter negotiates the full x4.

---

<br />

### Namespace test setup

Two NICs on the same host confuse Linux routing/ARP — must use network namespaces for clean point-to-point loop tests:

```bash
# Create namespace and move one interface into it
ip netns add ib_test
ip link set enp9s0 netns ib_test

# Configure inside namespace
ip netns exec ib_test ip addr add 198.18.0.2/30 dev enp9s0
ip netns exec ib_test ip link set enp9s0 up
ip netns exec ib_test ip link set enp9s0 mtu 9000

# Configure host side
ip addr add 198.18.0.1/30 dev ens1
ip link set ens1 up
ip link set ens1 mtu 9000

# Validate
ping -c 5 -M do -s 8972 198.18.0.2
```

Use `198.18.0.0/15` (RFC 2544 benchmark/testing range) instead of random RFC1918 for a clean "this is lab-only" address plan.

---

<br />

### What this setup is and isn't for

**Fine for:**
- Ethernet throughput testing (iperf3)
- Basic stability / heat tests
- Driver/firmware behavior
- DAQ-style packet work, mirroring, lab plumbing
- RDMA verbs testing, IPoIB experiments

**Not fine for:**
- Validating true 40GbE line-rate performance
- Stressing the card at its real ceiling
- Anything requiring predictable high bandwidth through the M.2 path
