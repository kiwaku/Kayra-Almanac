---
title: Hephaestus
id: hephaestus
year: 2025
domain: ["hardware", "os", "ai"]
status: "active"
featured: false
summary: "<strong>Dual-socket Xeon</strong> lab machine for adversarial network research and trusted compute. Platform integrates GPU passthrough, 40 Gb/s InfiniBand fabric, and VM isolation for security simulations.<ul><li>Tesla M40 (24GB VRAM) with VFIO</li><li>InfiniBand loop via 2× ConnectX-3 HCAs</li><li>Proxmox VE with attacker/victim/sensor VMs</li></ul>"
artifacts:
  diagram: "/artifacts/hephaestus/network_topology.png"
  diagram2: "/artifacts/hephaestus/pcie_power_map.png"
  readme: ""
images:
  - "/artifacts/hephaestus/overview.webp"
  - "/artifacts/hephaestus/topdown.webp"
  - "/artifacts/hephaestus/dual_cpu.webp"
related: ["ferrum-i", "ferrum-ii", "moltbot"]
---




Hephaestus Lab is a self-contained experimental compute and networking environment built inside a single workstation chassis. It combines dual Xeon E5-2650 v3 processors, a Tesla P40 for GPU-accelerated workloads, two ConnectX-3 adapters for a dedicated 40 Gb/s InfiniBand fabric, and an Intel I350-T4 for controlled Ethernet experiments. This hardware supports attacker, victim, and sensor virtual machines, traffic looping and mirroring, and direct host–guest data movement through VFIO passthrough.

The system is designed not as a general-purpose homelab, but as a focused research platform for adversarial simulations, GPU-based packet analysis, and studies of host/device boundaries. Its structure emphasizes compact, high-intensity capabilities—CUDA compute, point-to-point InfiniBand links, and isolated monitoring paths—rather than scale. Future extensions, such as integrating a BlueField card as a hidden co-processor, fit this same direction: examining how tightly coupled subsystems behave under stress, how communication channels can be exposed or obscured, and how constrained environments can support complex red-team and blue-team workflows.

---

<br />

### At a glance

- **Platform:** Dual-socket X99/C612, 2× Xeon E5-2650 v3, 32 GB DDR4 ECC RDIMM  
- **PCIe layout (Throughput Mode / Slot Map A):**
  - **PCIe x16_1 (CPU lanes)** **->** **Tesla P40**
  - Power: **PCIe 8‑pin** (dedicated cable from PSU)
  - **PCIe x16_2 (CPU lanes)** **->** **ConnectX‑3 HCA #1**
  - No aux power; ensure direct airflow across the heatsink
  - **M.2 (STRONG, Gen3×4) **->** M.2->x16 powered riser **->** ConnectX‑3 HCA #2**
  - Riser power: **PCIe 6‑pin** from PSU
  - **PCIe x1** **->** x16 riser A (powered) **->** Intel I350‑T4**
  - Riser power: **PCIe 6‑pin** from PSU
  - **PCIe x1 -> x16 riser B (powered) -> Quadro P400** (slot‑powered)
  - Riser power: **PCIe 6‑pin** from PSU
  - **M.2 (WEAK, PCH path) -> empty** (or low‑traffic device / opensm host only)
- **Networking planes:**
  - **Gigabit lab plane:** I350-T4 looped through a NETGEAR GS105E v2 (SPAN/VLAN/QoS tricks)
  - **Management/WAN:** USB Wi-Fi dongle, kept out-of-band from lab traffic
  - **Fabric plane:** 2×40 Gb/s InfiniBand (dual ConnectX-3, internal loop via QSFP+ DACs)
- **Virtualization:** Proxmox VE with dedicated VMs:
  - CUDA-box VM (P40 passthrough)
  - IB-box VM (HCA #2 passthrough)
  - Attacker / Victim / Sensor / Mgmt VMs on bridged NICs

---

<br />

## **Diagram 1 — Multi-Plane Virtual Networking Topology**

  <div class="diagram-section">
  <a href="/artifacts/hephaestus/network_topology.png" class="lightbox">
  <img src="/artifacts/hephaestus/network_topology.png" alt="Network topology" />
  </a>
  </div>

This diagram illustrates the allocation of physical network interfaces and virtual bridges into three distinct operational planes, each isolated for specific functions:

1. **Gigabit Ethernet Lab Plane**  
   An Intel I350-T4 quad-port NIC connects via port 1 to a Netgear GS105E v2 smart switch configured with two internal Cat6 loops (ports 1↔2 and 3↔4). This creates a controlled Layer 2 environment for security research, supporting ARP/DHCP manipulation, VLAN hopping, and traffic capture scenarios. The `vmbr0` bridge backhauls this physical interface to virtual machines (attacker, victim, sniffer, and management VMs), enabling realistic network attack simulations without affecting production infrastructure.

2. **Management/WAN Plane**  
   An ALFA AWUS036NEH USB Wi-Fi adapter provides the sole Internet connectivity for the Proxmox host, establishing a dedicated management path isolated from experimental traffic. This separation ensures secure remote access, package updates, and data transfer while maintaining the lab's air-gapped nature. Optional NAT routing through the host allows management VMs controlled Internet access without compromising lab isolation.

3. **InfiniBand Fabric Plane**  
   Two ConnectX-3 HCAs form a dual-link 40 Gb/s InfiniBand fabric via independent QSFP+ DAC cables (port1↔port1 and port2↔port2). The host retains HCA1 with `opensm` running for subnet management, while HCA2 is passed through to a dedicated VM via VFIO. IP-over-InfiniBand (IPoIB) subnets (10.0.40.0/24 and 10.0.41.0/24) provide high-speed connectivity for RDMA protocol testing and simulated storage backplane evaluation.

**Virtual Network Architecture:**  
- `vmbr0`: Lab domain bridged to I350 port1, carrying all experimental VM traffic
- `vmbr1`: Internal software bridge for VM-to-VM control traffic and segmentation
- **VM Assignments:** Attacker/victim/sniffer VMs connect to both bridges; CUDA-box VM uses only `vmbr1`; IB-box VM combines `vmbr1` management with direct HCA2 passthrough

---

<br />

## **Diagram 2 — Slot & Power Map (Throughput Mode)**

  <div class="diagram-section">
  <a href="/artifacts/hephaestus/pcie_power_map.png" class="lightbox">
  <img src="/artifacts/hephaestus/pcie_power_map.png" alt="PCIe power map" />
  </a>
  </div>

This hardware diagram details the physical layout, PCIe allocation, and power distribution for the dual-socket X99/C612 platform, emphasizing electrical safety and performance considerations:

**Core Components:**
- **CPU:** 2× Xeon E5-2650 v3 (10-core, 40 PCIe lanes each) with QPI interconnect
- **RAM:** 32 GB DDR4 ECC RDIMM (shared memory domain)
- **Storage:** Single 2.5" SATA SSD on SATA_0 (Proxmox OS + VM storage)
- **PSU:** Corsair HX1000i with precise rail allocation

**PCIe Slot Map & Bandwidth:**
1. **PCIe x16_1 (CPU1):** Tesla P40 (compute GPU) -> PCIe 3.0 ×16 (126 Gb/s)
2. **PCIe x16_2 (CPU2):** ConnectX-3 HCA1 (host IB) -> PCIe 3.0 ×8 (63 Gb/s)
3. **M.2 STRONG (CPU1):** ConnectX-3 HCA2 via M.2->x16 riser -> PCIe 3.0 ×4 (31.5 Gb/s)  
   *Note: Potential bottleneck for dual 40GbE IB (80 Gb/s theoretical)*
4. **PCIe x1 Slot A (PCH):** I350-T4 via ×1->x16 riser -> PCIe 2.0 ×1 (4 Gb/s)  
   *Note: Electrical ×1 limits quad 1GbE NIC to ~3.9 Gb/s aggregate*
5. **PCIe x1 Slot B (PCH):** Quadro P400 (display GPU) via ×1->x16 riser -> PCIe 2.0 ×1

**Power Distribution (Critical Safety Reference):**
- **CPU Power:** 2× EPS 8-pin to motherboard
- **PCIe Device Power:**
  - P40: PCIe 8-pin (6+2) connector
  - HCA2 riser: PCIe 6-pin #1
  - I350 riser: PCIe 6-pin #2  
  - P400 riser: PCIe 6-pin #3
- **Accessory Power:**
  - SATA power: SSD + fan hub
  - Molex: Dedicated to 12V->24V step-up converter for P40 blower fan  
    *Critical: Never connect blower directly to PSU*

**VFIO Device Assignment:**
- **Host Retains:** P400 (console), HCA1 (IB fabric), I350 (lab NIC), USB Wi-Fi
- **CUDA-box VM:** Tesla P40 via full device passthrough
- **IB-box VM:** ConnectX-3 HCA2 via full device passthrough
- **Remaining VMs:** VirtIO network interfaces only (no direct hardware access)

**Assembly Notes:**  
- Powered risers provide additional 12V rail but do not increase PCIe lane count
- M.2 STRONG provides only ×4 lanes despite x16 physical slot
- P40 requires dedicated cooling via 24V blower (separate power path)
- Display and compute GPU separation essential for VFIO compatibility
- Dual IB links maintain redundancy but HCA2 bandwidth limited by ×4 interface
---

<br />

### What this box is actually for

The design of Hephaestus emphasizes flexibility in network and system topology, allowing configurations to be adjusted as needed rather than fixed.

From the capabilities map: 
### Capabilities map (what this machine can do)

This map is split into two layers:
- **Core (A–G):** capabilities supported by the current hardware topology.
- **Stretch (H–M):** expansion axes and optional research directions (some require additional tooling/hardware).

---

#### Core capabilities (A–G)

<details className="capability">
  <summary><span className="chevron">▸</span><strong>A) Tap/Mirroring &amp; L2/L3 labs</strong> (I350-T4 + NETGEAR GS105E v2)</summary>

- SPAN mirroring: mirror p1→p5 (sniffer) while traffic flows p1↔p2 (victim path)
- Dual-path setups: p1↔p2 primary, p3↔p4 alternate; mirror either path
- VLAN labs (802.1Q): tag/untag per port; trunk to router VM; test ACLs/NAT between VLANs
- Rate-limit / QoS: police egress; validate markings and priority behavior in captures
- Multicast behavior: IGMP snooping vs flood; detect abusive hosts
- Loop/storm drills: create and mitigate broadcast storms; measure detector response
- MTU fault injection: jumbo-frame mismatches; validate PMTUD/fragment handling
- Cable diagnostics: GS105E cable test to validate physical-layer fault injections
- Inline MITM (physical): insert switch in-path; SPAN both directions into a sensor VM
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>B) CUDA/Compute</strong> (Tesla P40 passthrough)</summary>

- GPU-accelerated IDS/analytics: feature extraction, clustering, anomaly scoring on mirrored traffic
- Isolated attack workloads: GPU cracking/fuzzing inside an attacker VM; observe DMA/IOMMU boundaries
- Adversarial ML loops: generate and evaluate adversarial payloads with PCAP replay
- vGPU exploration (Pascal-era): multi-tenant CUDA experiments (where supported)
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>C) InfiniBand/RDMA fabric</strong> (dual ConnectX-3 loop)</summary>

- Self-contained 40 Gb/s fabric: private "datacenter backbone" via IPoIB
- RDMA verbs testing: latency/throughput microbench, congestion, starvation scenarios
- Protocol flips: validate stacks under IB/Eth/RoCE mode changes
- RDMA storage: iSER/SRP targets; SAN-like behavior for VMs
- GPU feed path (experimental): IB → CUDA data paths on compatible stacks
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>D) Storage topologies</strong> (SATA now; upgradeable)</summary>

- SATA pool: host OS + snapshots + light VM I/O
- PCAP spool tiers: SSD hot buffer → longer-term archive storage (as storage expands)
- ZFS tuning: SLOG/recordsize/ARC stress for PCAP vs DB workloads
- Self-encrypting disk drills: Opal/SED workflows for incident-response practice
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>E) VFIO/IOMMU/PCIe boundary testing</strong></summary>

- GPU reset/isolation: stable detach/reattach; cold/warm reset edge cases
- IOMMU grouping constraints: map isolation boundaries; validate what can/can't be cleanly separated
- MMIO / device surface: translate "PCIe as attack surface" from theory to practice
- Riser power integrity: observe stability under controlled load changes (with safe dummy loads)
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>F) Replay & traffic engineering</strong></summary>

- Time-accurate replay: tcpreplay + analysis pipelines; IDS evasion via pacing/ordering
- Clock skew studies: host vs VM timestamp drift; NIC/OS jitter quantification
- Burst stress: jumbo vs tiny-burst packet trains to hit driver/buffer edge cases
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>G) Security research</strong> (red/blue loop)</summary>

- End-to-end loop: Attacker → (I350 loops) → Victim; SPAN → Sensor; optional Defender (CUDA analytics)
- Protocol abuse labs: LLMNR/NetBIOS/MDNS poisoning, DHCP starvation, ARP cache games
- TLS visibility trade-offs: CPU relief vs inspection blind spots (software-first; accelerators later)
- Payload mutation loops: GPU-assisted variation + sensor hardening against evolving variants
</details>

---

#### Stretch axes (H–M)

<details className="capability">
  <summary><span className="chevron">▸</span><strong>H) Monitoring / observability</strong> (optional)</summary>

- Out-of-band capture: dedicated sniffer VM; optional hardware TAP later
- Host/guest tracing: perf/eBPF to link drops to scheduler/IRQ behavior
- Synthetic fault injection: scripted loss/reorder/latency to validate detectors
- Power/thermal telemetry: correlate rails/fan curves with stability and packet loss
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>I) Topology as a variable</strong> (roadmap)</summary>

- PCIe fan-out: hang accelerators/storage behind a switch; move display to x1 to free lanes
- Externalized PCIe: removable endpoints for "malicious device" simulations
- SR-IOV (where supported): per-VM vNIC isolation and policy enforcement tests
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>J) Optional add-ons</strong> (expansion axes)</summary>

- Intel QAT: TLS/IPsec offload at scale without pegging CPUs
- DMA attacker cards: validate or break IOMMU isolation with controlled drills
- SmartNIC / P4 NICs: on-NIC parsing/filtering; pre-disk reduction; line-rate surgery
- Precision timing: PTP/GPSDO to correlate captures across VMs at sub-µs scale
- PCIe SDR: RF ingress bridged into the same tap/replay workflows
- HSM: hardware keys, signed capture chains, tamper-evident storage
- FPGA NICs: custom parsers/flow meters for niche protocols
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>K) Physical & power experiments</strong> (optional)</summary>

- Cooling regimes: quantify NIC/GPU error rates vs airflow/temperature
- 12→24 V conversion behavior: EMI/noise effects on stability; cable routing best practices
- Patch ergonomics: rapid rewiring to measure how topology changes affect detection
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>L) Why it matters</strong> (one-box datacenter)</summary>

- Cloud-like attack/defend lifecycles without a rack
- Treat NICs/GPUs as untrusted co-processors; build evidence for/against covert channels
- Convert each capability into a reproducible lab module (portfolio-grade)
</details>

<details className="capability">
  <summary><span className="chevron">▸</span><strong>M) Future: BlueField-2</strong> (roadmap)</summary>

- In-line middlebox: firewall/NAT/IDS on the NIC; host sees "cleaned" flows
- Co-host trust boundary: ARM node with DMA access; supply-chain/firmware threat modeling
- NVMe-oF / iSCSI on DPU: block targets presented without waking CPUs
- P2P offload: DPU↔GPU or DPU↔IB experiments; CPU savings vs blind spots
- "Hostile DPU" posture: SPAN + strict ACLs + firmware fingerprinting as a research artifact
</details>

This framework supports the development of standardized experimental modules over time, such as those focused on port mirroring and VLAN management, RDMA-based storage area networks, VFIO reliability assessments, and GPU-supported defensive analytics.

---

<br />

### Build notes & logs

Detailed hardware specifications and debug logs:

- [Hardware documentation](/hardware) – BOM, wiring diagrams, and power distribution
- [Debug logs](/notebooks#hephaestus-debug) – Build issues, Q-codes, and troubleshooting